At first, we need a connector for our beloved crawler.

>>> import zeit.connector.mock
>>> connector = zeit.connector.mock.Connector()
>>> connector
<zeit.connector.mock.Connector object at 0x...>

Lookup for resource

>>> resource = connector[
...     'http://xml.zeit.de/online/2007/01/4schanzentournee-abgesang']
>>> type(resource)
<class 'zeit.connector.resource.Resource'>

Get XML Data

>>> xml = resource.data.read()
>>> print xml
<?xml ...
...
</article>
...

Import the converter

>>> from zeit.care.divisor import Converter
>>> Converter
<class 'zeit.care.divisor.Converter'>

Konvertiere das XML nach neuem format mit divisions

>>> new_xml = Converter(xml).convert()
>>> print new_xml
<?xml ...

Check conversion

>>> from lxml import etree
>>> import StringIO, os
>>> tree = etree.parse(StringIO.StringIO(new_xml))
>>> for d in tree.xpath('//body/division[@type="page"]'):
...     print len(d.xpath('p'))
6
0

Create test collection

>>> from zeit.connector.resource import Resource
>>> coll_path = 'http://xml.zeit.de/testdocs/'
>>> col = Resource(coll_path,
...                    'testdocs',
...                    'collection',
...                    StringIO.StringIO(''))
>>> connector.add(col)  
>>> connector[coll_path].type
'collection'


Get Testdocs

>>> testdir = os.path.dirname(__file__)+'/testdocs/'
>>> testdocs = [(f,testdir+f) for f in os.listdir(testdir) if os.path.isfile(testdir+f)]
>>> len(testdocs) > 0
True

Create test articles 

>>> for (filename, filepath) in testdocs:
...     res = Resource('http://xml.zeit.de/testdocs/'+filename,
...                filename,
...                'article',
...                open(filepath),
...                contentType = 'text/xml')
...     connector.add(res)

check the new ressources

>>> for (filename, filepath) in testdocs:
...     'http://xml.zeit.de/testdocs/'+filename in connector
True
True
True
...

Convert all test resources
>>> from zeit.care.divisor import division_worker
>>> for (filename, filepath) in testdocs:
...     division_worker(connector['http://xml.zeit.de/testdocs/'+filename], connector)
>>> for (filename, filepath) in testdocs:
...     resource = connector['http://xml.zeit.de/testdocs/'+filename]
...     if resource.type == 'article':
...         new_xml = resource.data.read()
...         tree = etree.parse(StringIO.StringIO(new_xml))
...         if tree.getroot().tag == 'article':
...             len(tree.xpath('//body/division[@type="page"]')) > 0
True
True
True
True
...

Check for some special cases:

>>> res = connector['http://xml.zeit.de/testdocs/klimawandel-usa']
>>> tree = etree.parse(StringIO.StringIO(res.data.read()))
>>> para = tree.xpath('//body/division[@type="page"][2]')[0].getchildren()[0]
>>> print etree.tostring(para, method="text", encoding="utf-8")
Durch steigende Meeresspiegel könnten die Inselgruppe der Florida Keys ...

>>> res = connector['http://xml.zeit.de/testdocs/Rissforschung']
>>> tree = etree.parse(StringIO.StringIO(res.data.read()))
>>> para = tree.xpath('//body/division[@type="page"][2]')[0].getchildren()[0]
>>> print etree.tostring(para, method="text", encoding="utf-8")
Vorbei. Die Idee von der Dauerfestigkeit ...

>>> res = connector['http://xml.zeit.de/testdocs/Zukunft-USA']
>>> tree = etree.parse(StringIO.StringIO(res.data.read()))
>>> para = tree.xpath('//body/division[@type="page"][2]')[0].getchildren()[0]
>>> print etree.tostring(para, method="text", encoding="utf-8")
Die Infrastruktur bröckelt. Überall sehen Amerikaner Verfall: ausgefahrene Straßen,...

>>> res = connector['http://xml.zeit.de/testdocs/patrick-wolf-interview']
>>> tree = etree.parse(StringIO.StringIO(res.data.read()))
>>> para = tree.xpath('//body/division[@type="page"][3]')[0].getchildren()[0]
>>> print etree.tostring(para, method="text", encoding="utf-8")
ZEIT ONLINE: Den natürlichen Folk-Klängen setzen Sie synthetische gegenüber. ...

>>> res = connector['http://xml.zeit.de/testdocs/Konjunkturprogramm']
>>> tree = etree.parse(StringIO.StringIO(res.data.read()))
>>> tree.xpath('//body/division[@type="page"][2]')[0]
<Element division at ...
>>> tree.xpath('//body/division[@type="page"][3]')
[]